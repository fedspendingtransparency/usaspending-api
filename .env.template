########################################################################################################################
# [TEMPLATE FOR] USER specific ENVIRONMENT variables for local environment
# After copying this file to ./.env these ENV VAR values will be read-in in two places
# (1) docker configuration
#     - Variables defined here will be substituted for variables like ${THIS_VAR} that exist in docker-compose.yml
#     - To see the interpolated config values in use, run:
#       > docker-compose config
#       or
#       > make docker-compose-config
# (2) Runtime env configuration via usaspending_api.config.* modules
#     - Variables defined here will override variables of the same name in default or env-specific
#       config data classes (e.g. DefaultConfig in default.py and/or LocalConfig local.py)
########################################################################################################################
# ==== [App] ====
# MATVIEW_SQL_DIR has to be inside of the project (check the docker-compose file)
MATVIEW_SQL_DIR=matview_sql
PROJECT_LOG_DIR=./usaspending_api/logs

# ==== [Django] ====
# Use env var to toggle Django's DEBUG mode for local docker envs
DJANGO_DEBUG=False

# ==== [Postgres] ====
# Change POSTGRES_HOST to host.docker.internal if you are running a local Postgres server on the host machine
# Otherwise leave as-is, so other docker containerized services will use the Postgres created by Compose.
POSTGRES_HOST=usaspending-db
POSTGRES_PORT=5432
POSTGRES_USER=usaspending
POSTGRES_PASSWORD=usaspender

# Configuration values for a connection string to a Broker database
#    Only necessary for some management commands
BROKER_USER=root
BROKER_PASSWORD=password
BROKER_HOST=broker-db
BROKER_PORT=5432

# ==== [Elasticsearch] ====
# Where to connect to elasticsearch.
# Should include scheme (http:// or https://, host, and port (if different than 80 or 443 for HTTP/HTTPS)
ES_HOST=http://usaspending-es
ES_PORT=9200

# ==== [Spark] ====
SPARK_MASTER_PORT=7077
SPARK_MASTER_WEBUI_PORT=4040
SPARK_HISTORY_SERVER_PORT=18080
# Should point to a path where data can be persisted beyond docker restarts, outside of the git source repository
# The specified directory needs to exist before Docker can mount it
SPARK_CLUSTER_DATA_DIR=${HOME}/Development/data/usaspending/docker/usaspending-spark
# Set to True to have the tests store the Spark data in your filesystem (and not MinIO)
USE_FILESYSTEM_FOR_SPARK_DATA=False

# ==== [AWS] ====
# Overriding AWS_PROFILE fron ?USER_SPECIFIC_OVERRIDE? to empty string so it won't be used. Set value here if needed.
AWS_PROFILE=

# ==== [MinIO] ====
MINIO_PORT=9000
MINIO_CONSOLE_PORT=9001
# Should point to a path where data can be persisted beyond docker restarts, outside of the git source repository
# The specified directory needs to exist before Docker can mount it
MINIO_DATA_DIR=${HOME}/Development/data/usaspending/docker/usaspending-s3
